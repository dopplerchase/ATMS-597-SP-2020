{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5 Classification Group E\n",
    "\n",
    "Randy , Rylan , Alex\n",
    "\n",
    "If you would like to see the output of the code, please see the other individual files uploaded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First bit, download and clean up \n",
    "\n",
    "Note, this code is written for keeling not google colab (needed extra processors to get it to run more quickly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metpy.io import parse_metar_to_dataframe\n",
    "import pandas as pd\n",
    "import wget\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "import glob \n",
    "\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "def padder(x):\n",
    "    if x < 10:\n",
    "        x = '0' + str(x)\n",
    "    else:\n",
    "        x = str(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def download_par(item):\n",
    "    try:\n",
    "        ff=wget.download(item,'./raw_data/'+item[-14:])\n",
    "    except: \n",
    "        pass \n",
    "    \n",
    "    return\n",
    "\n",
    "def decorde_par(item):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "        timestamp = pd.to_datetime(np.loadtxt('timestamp.info',dtype=str)[0])\n",
    "        try:\n",
    "            d = parse_metar_to_dataframe(item.values[0][52:], year=timestamp.year, month=timestamp.month)\n",
    "            return d\n",
    "        except:\n",
    "            print('Error with METAR: ', item.values[0][52:])\n",
    "\n",
    "\n",
    "class ASOS():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = True\n",
    "\n",
    "    def grab_datafiles(self,station = None,parallel=True,overwrite=True):\n",
    "\n",
    "        if station is None:\n",
    "            print('please provide a station')\n",
    "            \n",
    "        from ftplib import FTP\n",
    "        import os\n",
    "\n",
    "        ftp =  FTP('ftp.ncdc.noaa.gov') # ftp access to ncdc.noaa.gov\n",
    "        ftp.login() # anonymous ftp login\n",
    "        ftp.cwd('pub/data/asos-fivemin/') # change directory to asos-onemin\n",
    "        dirs = ftp.nlst() \n",
    "        dirs.sort()\n",
    "        dirs = dirs[0:-9]\n",
    "        file_list = []\n",
    "        sub = station\n",
    "        for j in dirs:\n",
    "            ftp.cwd('/pub/data/asos-fivemin/' + j)\n",
    "            data_files = ftp.nlst()\n",
    "            for s in filter (lambda x: sub in x, data_files): file_list.append('ftp://ftp.ncdc.noaa.gov/pub/data/asos-fivemin/'+ j + '/' + s)\n",
    "\n",
    "        ftp.close()\n",
    "\n",
    "        url_strs = file_list\n",
    "        self.urls = url_strs\n",
    "        ####################################################\n",
    "        if os.path.isdir('./raw_data/') is False:\n",
    "            os.mkdir('./raw_data/')\n",
    "\n",
    "        if parallel:\n",
    "            import multiprocessing as mp\n",
    "            pool = mp.Pool(processes=2)\n",
    "            for _ in tqdm(pool.imap_unordered(download_par, url_strs), total=len(url_strs)):\n",
    "                  pass\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        else:\n",
    "            for item in tqdm(url_strs):\n",
    "                ff=wget.download(item,'./raw_data/'+item[-14:])\n",
    "\n",
    "    def process_file(self,filename=None,parallel=True,save=True):\n",
    "        if os.path.isdir('./decoded_data/') is False:\n",
    "            os.mkdir('./decoded_data/')\n",
    "\n",
    "        df = pd.read_csv(filename,header=None)\n",
    "        # Pull the timestamp from the filename\n",
    "        timestamp = datetime.datetime.strptime(filename[-10:], '%Y%m.dat')\n",
    "\n",
    "        iterrows = list(df.iterrows())\n",
    "        index, rows = zip(*iterrows)\n",
    "\n",
    "        logfile = open('timestamp.info', \"w\")\n",
    "        logfile.write(str(timestamp))\n",
    "        logfile.close()\n",
    "\n",
    "        df_list = []\n",
    "\n",
    "        if parallel:\n",
    "            import multiprocessing as mp\n",
    "            pool = mp.Pool(processes=10)\n",
    "            for d in tqdm(pool.imap(decorde_par, rows), total=len(rows)):\n",
    "                  df_list.append(d)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        else:\n",
    "            timestamp = pd.to_datetime(np.loadtxt('timestamp.info',dtype=str)[0])\n",
    "            for item in tqdm(rows):\n",
    "                try:\n",
    "                    df_list.append(parse_metar_to_dataframe(item.values[0][52:], year=timestamp.year, month=timestamp.month))\n",
    "                except:\n",
    "                    print('Error with METAR: ', item.values[0][52:])\n",
    "\n",
    "        df_master = pd.concat(df_list)\n",
    "        df_master = df_master.sort_values('date_time')\n",
    "\n",
    "        if save:\n",
    "            df_master.to_csv('./decoded_data/' + filename[-10:-4] + '.csv')\n",
    "\n",
    "        self.df_master = df_master \n",
    "\n",
    "\n",
    "    def process_all_files(self):\n",
    "        files = glob.glob('./raw_data/*.dat')\n",
    "        files.sort()\n",
    "        for i in files[151:]:\n",
    "            print(i)\n",
    "            if i == './raw_data/KIMT201608.dat':\n",
    "                continue\n",
    "            self.process_file(filename=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab and download \n",
    "asos = ASOS()\n",
    "asos.grab_datafiles(station='KIMT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./raw_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process\n",
    "asos.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat all files\n",
    "files = glob.glob('./decoded_data/*')\n",
    "files.sort()\n",
    "df_list = []\n",
    "for i in files:\n",
    "    df_list.append(pd.read_csv(i))\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df = df.drop(columns=['air_pressure_at_sea_level','low_cloud_type','high_cloud_level', 'highest_cloud_type','highest_cloud_level','low_cloud_level', 'medium_cloud_level', 'high_cloud_type','current_wx1', 'current_wx2', 'current_wx3','Unnamed: 0','station_id','latitude','longitude','cloud_coverage','past_weather','medium_cloud_type','eastward_wind','northward_wind','past_weather2'])\n",
    "df = df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc some params.\n",
    "altim = df.altimeter.values * units.in_Hg\n",
    "height = df.elevation.values * units.meter\n",
    "temp = df.air_temperature.values *units.degC\n",
    "dew = df.dew_point_temperature.values *units.degC\n",
    "slp = mpcalc.altimeter_to_sea_level_pressure(altim,height,temp)\n",
    "p = mpcalc.altimeter_to_station_pressure(altim,height)\n",
    "p = p.to(units.hectopascal)\n",
    "slp = slp.to(units.hectopascal)\n",
    "slp = np.asarray(slp,dtype=float)\n",
    "p = np.asarray(p,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab only weather times\n",
    "df_timeseries = pd.DataFrame({'wind_direction':df.wind_direction.values,'wind_speed':df.wind_speed.values,'air_temperature':df.air_temperature.values,'dew_point_temperature':df.dew_point_temperature.values,'present_weather':df.present_weather.values,'sea_level_pressure':slp,'pressure':p},index=pd.to_datetime(df.date_time.values))\n",
    "date_indexs = pd.date_range(df_timeseries.index.values.min(),df_timeseries.index.values.max(),freq='300S')\n",
    "df_timeseries = df_timeseries[~df_timeseries.index.duplicated()]\n",
    "df_timeseries = df_timeseries.reindex(index=date_indexs)\n",
    "df_nowx = df_timeseries.where(df_timeseries.present_weather == 0).dropna(how='all')\n",
    "df_wx = df_timeseries.where(df_timeseries.present_weather > 0).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc wet bulb when there is weather \n",
    "zipped = list(zip(df_wx.pressure.values,df_wx.air_temperature.values,df_wx.dew_point_temperature.values,np.arange(0,len(df_wx.dew_point_temperature.values))))\n",
    "\n",
    "def wet_bulb_par(item):\n",
    "    try:\n",
    "        tw = mpcalc.wet_bulb_temperature(item[0:1]*units.hectopascal, item[1:2]*units.degC, item[2:3]*units.degC)\n",
    "    except:\n",
    "        tw = np.nan\n",
    "    return [tw,item[3]]\n",
    "\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "pool = mp.Pool(processes=24)\n",
    "tw_list = []\n",
    "for d in tqdm(pool.imap(wet_bulb_par, zipped), total=len(zipped)):\n",
    "      tw_list.append(d)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "lst1, lst2 = zip(*tw_list)\n",
    "ttt = np.zeros(len(lst1))\n",
    "#force the correct index \n",
    "for i,ii in enumerate(lst1):\n",
    "    ttt[lst2[i]] = np.asarray(lst1[i])\n",
    "    \n",
    "df_wx['wet_bulb'] = pd.Series(ttt, index=df_wx.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "snow = [70,71,72,73,74,75,76,77,78,79,85,86]\n",
    "rain = [50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,80,81,82]\n",
    "\n",
    "df_list = []\n",
    "for i in snow:\n",
    "    df_list.append(df_wx.where(df_wx.present_weather == i).dropna(how='all'))\n",
    "    \n",
    "df_snow=pd.concat(df_list)\n",
    "\n",
    "df_list = []\n",
    "for i in rain:\n",
    "    df_list.append(df_wx.where(df_wx.present_weather == i).dropna(how='all'))\n",
    "    \n",
    "df_rain=pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a gander at data \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.hist(df_snow.present_weather,bins=np.arange(50,90),color='dodgerblue',edgecolor='k',label='Frozen Precip.')\n",
    "plt.hist(df_rain.present_weather,bins=np.arange(50,90),color='orangered',edgecolor='k',label='Liquid Precip.')\n",
    "plt.semilogy()\n",
    "plt.legend(loc=1)\n",
    "plt.title('KIMT, Iron Mountain MI, 2005 - 2019')\n",
    "plt.xlabel('Present Weather Code')\n",
    "plt.ylabel('Count')\n",
    "plt.text(48.5,5e4,'n_samples : {}'.format(df_snow.present_weather.shape[0]),color='dodgerblue')\n",
    "plt.text(48.5,3e4,'n_samples : {}'.format(df_rain.present_weather.shape[0]),color='orangered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Logistic Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dopplerchase/ATMS-597-SP-2020.git\n",
    "rain = pd.read_csv('./ATMS-597-SP-2020/ATMS-597-SP-2020-Project-5/RAIN_KIMT.csv')\n",
    "snow = pd.read_csv('./ATMS-597-SP-2020/ATMS-597-SP-2020-Project-5/SNOW_KIMT.csv')\n",
    "\n",
    "data = pd.concat([rain,snow])\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which present weather codes are in our data\n",
    "data['present_weather'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change frozen/liquid precip to read as such \n",
    "data['present_weather'].replace(to_replace=[61.0, 63.0, 65.0, 66.0, 67.0, 71.0, 73.0, 75.0], value= ['Liquid', 'Liquid', 'Liquid', 'Liquid', 'Liquid', 'Frozen', 'Frozen', 'Frozen'], inplace=True)\n",
    "\n",
    "# Rename present weather column and drop unecessary columns\n",
    "data.rename(columns={'present_weather': 'Precipitation_Type'}, inplace = True)\n",
    "data.drop('Unnamed: 0', axis = 1, inplace= True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Precipitation_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with a Nan\n",
    "data.dropna(axis=0, how='any', inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Precipitation_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set liquid precip equal to 1, frozen to 0 \n",
    "data.Precipitation_Type = [1 if each == 'Liquid' else 0 for each in data.Precipitation_Type]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into features and target variable\n",
    "cols = ['wind_direction', 'wind_speed',\t'air_temperature', 'dew_point_temperature',\t'sea_level_pressure',\t'pressure',\t'wet_bulb']\n",
    "X = data[cols] #features\n",
    "y = data.Precipitation_Type # Target variable\n",
    "\n",
    "# Scale X matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "# Build and fit logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct confusion matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistics\n",
    "\n",
    "# Build array of length y_test with 48% ones, 52% zeroes for climo y_pred\n",
    "prob_array = np.zeros([100],dtype=int)\n",
    "prob_array[0:48] = 1\n",
    "np.random.shuffle(prob_array)\n",
    "y_pred2 = np.random.choice(prob_array,size=y_test.shape[0])\n",
    "\n",
    "# create variables of model and climo brier scores to calc brier skill score\n",
    "bs_m = metrics.brier_score_loss(y_test, y_pred)\n",
    "bs_c = metrics.brier_score_loss(y_test, y_pred2)\n",
    "bss = ((bs_c - bs_m)/bs_c)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Logistic Regression Brier Score:\", metrics.brier_score_loss(y_test, y_pred))\n",
    "print(\"Climatology Brier Score:\", metrics.brier_score_loss(y_test, y_pred2))\n",
    "print(\"Brier Skill Score:\", bss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve \n",
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"Logistic Regression, auc=\"+str(auc))\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into features and target variable\n",
    "cols = ['wind_direction', 'wind_speed',\t'air_temperature', 'dew_point_temperature',\t'sea_level_pressure',\t'pressure',\t'wet_bulb']\n",
    "X2 = data[cols] #features\n",
    "y = data.Precipitation_Type # Target variable\n",
    "\n",
    "# scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X2 = scaler.fit_transform(X2)\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X2_train,X2_test,y_train,y_test = train_test_split(X2,y,test_size=0.3,random_state=0)\n",
    "\n",
    "# creating Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X2_train,y_train)\n",
    "gnb_y_pred = gnb.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_array = np.zeros([100],dtype=int)\n",
    "prob_array[0:49] = 1\n",
    "np.random.shuffle(prob_array)\n",
    "clim_y_pred = np.random.choice(prob_array,size=y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct confusion matrix\n",
    "from sklearn import metrics\n",
    "gnb_cnf_matrix = metrics.confusion_matrix(y_test, gnb_y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(gnb_cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, gnb_y_pred)\n",
    "brier_score = metrics.brier_score_loss(y_test, gnb_y_pred)\n",
    "brier_score_climo = metrics.brier_score_loss(y_test, clim_y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Brier Score:\", brier_score)\n",
    "print(\"Brier Score (Climo):\", brier_score_climo)\n",
    "print(\"Brier Skill Score:\", 1 - brier_score/brier_score_climo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
