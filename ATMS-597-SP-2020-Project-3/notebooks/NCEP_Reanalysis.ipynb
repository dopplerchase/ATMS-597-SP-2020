{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCEP_Reanalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPf-IRvKUEi7",
        "colab_type": "text"
      },
      "source": [
        "### Welcome to a notebook to get the Reanalysis data associated with Project 3\n",
        "\n",
        "Push this button to open it in Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/dopplerchase/ATMS-597-SP-2020/blob/master/ATMS-597-SP-2020-Project-3/notebooks/NCEP_Reanalysis.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZW-AwOpTxOq",
        "colab_type": "text"
      },
      "source": [
        "This part of code has two purposes:\n",
        "1) to take an array of dates with extreme precipitation (defined as \n",
        "above 95% values of daily precipitation during season JJA) in Champaign,\n",
        "IL and fetch following daily average data from NCEP/NCAR Reanalysis \n",
        "dataset for these days:\n",
        "\n",
        "    (1) wind vectors [surface, 250, 500, 850hPa] \n",
        "    (2) wind speed [250hPa] \n",
        "    (3) temperature [850hPa, skin] \n",
        "    (4) geopotential height [500hPa] \n",
        "    (5) specific humidity [850hPa]\n",
        "    (6) total atm column water vapor.\n",
        "\n",
        "2) fetch daily long term mean data (1981-2010) for above indices, \n",
        "average to seasonal mean for JJA, and calculate seasonal anomaly for\n",
        "the extreme precipitation days.\n",
        "\n",
        "The code returns two netCDF files:\n",
        "\n",
        "1) daily Reanalysis data for the selected extreme precipitation days,\n",
        "which is 101 days and 14 single-level variables, global.\n",
        "2) seasonal long term mean climate, 1 day, 14 single-level variables, \n",
        "global.\n",
        "\n",
        "A different version of code, which has less documentation but is \n",
        "integrated in a class, is attached at the end of the file (ln 251).\n",
        "ACKNOLEDGEMENT:\n",
        "NCEP Reanalysis data provided by the NOAA/OAR/ESRL PSD, Boulder, Colorado, \n",
        "USA, from their Web site at https://www.esrl.noaa.gov/psd/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blmg2EupUOC8",
        "colab_type": "text"
      },
      "source": [
        "### install needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYFgb9nT38V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install netCDF4 \n",
        "!pip install pydap\n",
        "!apt-get -qq install libproj-dev proj-data proj-bin libgeos-dev\n",
        "!pip install Cython\n",
        "!pip install --upgrade --force-reinstall shapely --no-binary shapely\n",
        "!pip install cartopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h6-pYhqUQQI",
        "colab_type": "text"
      },
      "source": [
        "### import needed modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxPsZJmT-Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYzg2fqkUTK-",
        "colab_type": "text"
      },
      "source": [
        "### import data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIEzIvJhUUXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date_file = './ATMS-597-SP-2020/ATMS-597-SP-2020-Project-3/data/precip_gte95quant.csv'\n",
        "dates_gte_95 = pd.read_csv(date_file, header=None, parse_dates=[1], index_col=[1]).index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg91GPc5Ua2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Daily composite for extreme precipitation days:\n",
        "# daily_select_data = 'https://drive.google.com/uc?export=download&id=1EGUjs0s6GzdgGw07Xn78t9ORUJdVt2CY'\n",
        "# Long term mean daily data for base period (1981-2010, JJA):\n",
        "# ltm_jja_mean_data = 'https://drive.google.com/uc?export=download&id=1-PyxlJH00ySZXly0AXeQ04R0fzUDE604'\n",
        "# Can download and upload to colab to avoid running the code below (~5 min)\n",
        "\n",
        "# If .nc files already in directory, run\n",
        "# daily_select = xr.open_dataset('daily_select.nc', engine='netcdf4')\n",
        "# ltm_JJA_mean = xr.open_dataset('ltm_JJA_mean.nc', engine='netcdf4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Le0I2YUerJ",
        "colab_type": "text"
      },
      "source": [
        "### Fetch Daily and Long Term Mean Reanalysis Data/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoSocQFuUdqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define a dictionary for file locations on NOAA thredds server\n",
        "baseurl = 'https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/'\n",
        "dailyurl = baseurl + 'ncep.reanalysis.dailyavgs/'\n",
        "ltmurl = baseurl + 'ncep.reanalysis.derived/'\n",
        "data_loc = {'uwnd_250' : ['pressure/uwnd.',\n",
        "                        '.sel(level = 250).drop(\\'level\\')'],\n",
        "            'uwnd_500' : ['pressure/uwnd.',\n",
        "                        '.sel(level = 500).drop(\\'level\\')'],\n",
        "            'uwnd_850' : ['pressure/uwnd.',\n",
        "                        '.sel(level = 850).drop(\\'level\\')'],\n",
        "            'vwnd_250' : ['pressure/vwnd.', \n",
        "                        '.sel(level = 250).drop(\\'level\\')'],\n",
        "            'vwnd_500' : ['pressure/vwnd.', \n",
        "                        '.sel(level = 500).drop(\\'level\\')'],\n",
        "            'vwnd_850' : ['pressure/vwnd.', \n",
        "                        '.sel(level = 850).drop(\\'level\\')'],\n",
        "            'hgt_500'  : ['pressure/hgt.', \n",
        "                        '.sel(level = 500).drop(\\'level\\')'],\n",
        "            'temp_skin': ['surface_gauss/skt.sfc.gauss.', ''],\n",
        "            'temp_500' : ['pressure/air.', \n",
        "                        '.sel(level = 500).drop(\\'level\\')'],\n",
        "            'shum_850' : ['pressure/shum.', \n",
        "                        '.sel(level = 850).drop(\\'level\\')'],\n",
        "            'uwnd_surf': ['surface/uwnd.sig995.', ''],\n",
        "            'vwnd_surf': ['surface/vwnd.sig995.', ''],\n",
        "            't_col_aq' : ['surface/pr_wtr.eatm.', '']} \n",
        "# Dictionary `data_loc`:\n",
        "  # Key: variable name to be stored in final `.nc` file\n",
        "  # Value: [0] final part of data url\n",
        "  #        [1] additional processing: select and dropping \n",
        "  #            unused levels, etc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXq50F2fUqOi",
        "colab_type": "text"
      },
      "source": [
        "### Download daily average data\n",
        "\n",
        "The following code was adapted from Prof. Nesbitt, available at\n",
        "https://colab.research.google.com/drive/1rAXpUlTdcUvCu4goKOYPwEtxB-fuk5CU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi0Uk5kcUnCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define range of years from `dates_gte_95`\n",
        "years = pd.date_range(start=dates_gte_95[0], end=dates_gte_95[-1], freq='AS')\n",
        "years\n",
        "\n",
        "# default resolution: 2.5 deg * 2.5 deg\n",
        "lat_grid = np.linspace(90, -90, 73)\n",
        "lon_grid = np.linspace(0, 360, 144, False)\n",
        "\n",
        "# Initialize empty list for datasets\n",
        "datasets = []\n",
        "\n",
        "# Loop through years. For each year, merge all data variables into one dataset.\n",
        "for iyr in years.year:\n",
        "    print('working on '+str(iyr))\n",
        "    dates = dates_gte_95[dates_gte_95.year == iyr]\n",
        "    # Initialize empty list for datasets, for all variable during this year `iyr`\n",
        "    year_dataset = []\n",
        "    for key in data_loc.keys():\n",
        "        url = dailyurl + data_loc.get(key)[0]\n",
        "        # Open dataset with Xarray, select dates and assign to temporary dateset\n",
        "        data = xr.open_dataset(url + str(iyr)+'.nc',engine='netcdf4').sel(time=dates)\n",
        "        # Execuate optional data processing defined in `data_loc` and rename variable\n",
        "        exec('data = data' + data_loc.get(key)[1] +\n",
        "             '.rename({\\'' + url.split('/')[-1].split('.')[0] + '\\':\\'' + key + '\\'})')\n",
        "        # Check resolution (grid) mismatch. Interpolate to default grid.\n",
        "        if size(data.lon) != size(lon_grid):\n",
        "            data = data.interp(lat=lat_grid, lon=lon_grid)\n",
        "        # Add temporary dateset to list of this year\n",
        "        year_dataset.append(data)\n",
        "    # Merge all variables of this year to one single dataset\n",
        "    ds = xr.merge(year_dataset)\n",
        "    # Append this year's dataset to grand list\n",
        "    datasets.append(ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEimqWftUwK6",
        "colab_type": "text"
      },
      "source": [
        "### Postprrocessing and save daily data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpc-G4ZFUvtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concat datasets of years to one dataset \n",
        "daily_select = xr.concat(datasets, dim='time')\n",
        "# Calculate Wind Speed Scalar at 250 hPa.\n",
        "daily_select = daily_select.assign(wspd_250 = np.sqrt(daily_select.uwnd_250**2 + \n",
        "                                                      daily_select.vwnd_250**2))\n",
        "\n",
        "# Check if there is the `.nc` file in the dir, if not save it. \n",
        "names = !ls daily*\n",
        "if not 'daily_select.nc' in str(names):\n",
        "    daily_select.to_netcdf('daily_select.nc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmkfqUlxU6Jt",
        "colab_type": "text"
      },
      "source": [
        "### Download daily long term mean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2f1nYPLU467",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize empty list for datasets\n",
        "ltm_dataset = []\n",
        "\n",
        "# Loop through variables and merge all data variables into one dataset.\n",
        "for key in data_loc.keys():\n",
        "    url = ltmurl + data_loc.get(key)[0].replace('gauss.','')\n",
        "    # Open dataset, drop unused variables\n",
        "    # `xr.open_dataset`: long term mean data has dates defined in year 0001, so\n",
        "    #                    specify `use_cftime = True`  \n",
        "    data = xr.open_dataset(url + 'day.1981-2010.ltm.nc',\n",
        "                           engine='netcdf4',\n",
        "                           use_cftime = True,\n",
        "                           drop_variables = ['climatology_bounds', 'valid_yr_count'])\n",
        "                           #.sel(time=slice('0001-06', '0001-08'))\n",
        "    # Select time (days in JJA season)\n",
        "    data = data.isel(time = data.time.dt.season == 'JJA')\n",
        "    # Execuate optional data processing defined in `data_loc` and rename variable \n",
        "    exec('data = data' + data_loc.get(key)[1] +\n",
        "         '.rename({\\'' + url.split('/')[-1].split('.')[0] + '\\':\\'' + key + '\\'})')\n",
        "    # Check resolution (grid) mismatch. Interpolate to default grid.\n",
        "    if size(data.lon) != size(lon_grid):\n",
        "        data = data.interp(lat=lat_grid, lon=lon_grid)\n",
        "    # Append single-variable datasets to a list\n",
        "    ltm_dataset.append(data)\n",
        "\n",
        "# Merge list of variables     \n",
        "ltm = xr.merge(ltm_dataset)\n",
        "# Calculate Wind Speed Scalar at 250 hPa.\n",
        "ltm = ltm.assign(wspd_250 = np.sqrt(ltm.uwnd_250**2 + \n",
        "                                    ltm.vwnd_250**2))\n",
        "# Check dataset\n",
        "# ltm\n",
        "\n",
        "# Calculate the average LTM daily data for JJA season ('global mean fields')\n",
        "ltm_JJA_mean = ltm.mean(dim='time')\n",
        "\n",
        "# Check if there is the `.nc` file in the dir, if not save it. \n",
        "names = !ls\n",
        "if not 'ltm_JJA_mean.nc' in str(names):\n",
        "    ltm_JJA_mean.to_netcdf('ltm_JJA_mean.nc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXApnPUXU73X",
        "colab_type": "text"
      },
      "source": [
        "### Calculate mean seasonal anomaly for extreme precipitation days (1 `time`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CENiUhO9U8Ox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "daily_select_mean = daily_select.mean(dim='time')\n",
        "seasonal_anomaly = daily_select_mean - ltm_JJA_mean\n",
        "\n",
        "## Calculate daily anomaly for extreme precipitation days (101 `time`)\n",
        "# Daily data for animation\n",
        "seasonal_anomaly_daily = daily_select - ltm_JJA_mean"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
